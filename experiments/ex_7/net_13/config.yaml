# General Parameters
checkpoint:              # none
log_name: ex7_net13.log  # name of the file with training info
gpu:                     # index of a GPU, if available
output_directory: experiments/ex_7/net_13 # the directory where to write the model file
parallel: false

model_path: "{}/{}_{}_{}.pth" # for those which use pretrained models

# Data Parameters
data:
  dataset: ImageNet   # MNIST and EMNIST dataset
  imagenet_path: /local/scratch/datasets/ImageNet/ILSVRC2012/ # ILSVRC2012 path
  train_file: protocols/p{}_train.csv        # relative to data directory
  val_file:   protocols/p{}_val.csv          # relative to data directory
  test_file:  protocols/p{}_test.csv         # relative to data directory


# Common parameters
seed: 42        # Common seed across all source of randomness
batch_size: 32  # If distributed training the batch size is multiplied by the number of gpus
epochs: 120   # Number of epochs to train.
workers: 4      # Dataloader number of workers
patience: 0    # Number of epochs to wait before stopping the training. 0 means no early stopping
unknown_for_training: False  # If True, the unknown class is used for training
unknown_in_both: False
remove_negative: False
# loss parameters
loss:
  type: bce  # either {entropic, softmax, garbage, bce}
  # Entropic Parameters
  w: 1.

# Optimizer Parameters
opt:
  type: adam  # Two options: {adam, sgd}
  lr: 1.e-3   # Initial learning rate
  decay: 0    # Number of epochs to wait for each learning rate reduction. 0 means no decay
  gamma: 1    # Factor to reduce the learning rate

# Algorithm parameters
algorithm:
  type: binary_ensemble_combined_imagenet
  num_models: 40
  sets: random # either random or hamming, if class distance should be optimized
  model: resnet50Plus # either resnet50 or resnet50Plus
