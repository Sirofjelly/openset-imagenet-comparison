# General Parameters
checkpoint:             # set the checkpoint from where to continue, leave empty to start from scratch
log_name: training_combined_emnist.log  # name of the file with training info
gpu:                    # index of a GPU, if available
output_directory: dev_experiments/EMNIST  # the directory where to write the model file
parallel: false

model_path: "{}/{}_{}_{}.pth" #for those which use pretrained models

# Data Parameters
data:
  dataset_path: /local/scratch/kuebler/ 
  dataset: emnist    # MNIST and EMNIST path

# Common parameters
seed: 42        # Common seed across all source of randomness
batch_size: 32  # If distributed training the batch size is multiplied by the number of gpus
epochs: 50    # Number of epochs to train.
workers: 0      # Dataloader number of workers
patience: 0    # Number of epochs to wait before stopping the training. 0 means no early stopping

# loss parameters
loss:
  type: sigmoid-focal  # either {entropic, softmax, garbage, bce, sigmoid-focal}
  # Entropic Parameters
  w: 1.

# Optimizer Parameters
opt:
  type: sgd  # Two options: {adam, sgd}
  lr: 1.e-3   # Initial learning rate
  decay: 0    # Number of epochs to wait for each learning rate reduction. 0 means no decay
  gamma: 1    # Factor to reduce the learning rate

# Algorithm parameters
algorithm:
  type: binary_ensemble_combined_emnist
  num_models: 100
